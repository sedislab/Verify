{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 1. VERIFY Dataset Exploration\n",
        "\n",
        "This notebook provides a basic walkthrough of loading and exploring the VERIFY dataset. We'll look at its structure, some basic statistics, and examples of the data.\n",
        "\n",
        "#### 1.1 Setup and Imports\n",
        "First, let's import the necessary libraries. We'll use `datasets` from Hugging Face to load the data (if available on the Hub), `pandas` for data manipulation, `matplotlib` and `seaborn` for plotting."
      ],
      "metadata": {
        "id": "byzjsD-RQXw7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qe6rWRvSQRW-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import load_dataset\n",
        "import sqlite3\n",
        "import json\n",
        "from collections import Counter\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_context(\"notebook\", font_scale=1.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2 Loading the Dataset\n",
        "\n",
        "The VERIFY dataset can be loaded either from the Hugging Face Hub or Kaggle or from GitHub or a local SQLite database file."
      ],
      "metadata": {
        "id": "ivFu-hhfQ4b_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HF_DATASET_NAME = \"sedislab/VERIFY\"\n",
        "\n",
        "# Option B: Local files (if cloned from GitHub: sedislab/verify)\n",
        "# Adjust these paths if your local repository structure is different.\n",
        "LOCAL_CSV_PATH = \"data/verify.csv\"\n",
        "LOCAL_PARQUET_PATH = \"data/verify.parquet\"\n",
        "\n",
        "# Option C: Local SQLite Database\n",
        "# Replace with the actual path to your database if using this method.\n",
        "DB_PATH = '/projects/path/to/verify-dataset/dataset.db' # Example path\n",
        "\n",
        "df = None"
      ],
      "metadata": {
        "id": "n8sMvtnTQ4Al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Option A: Loading from Hugging Face Hub\n",
        "downloaded_file_path = None\n",
        "\n",
        "downloaded_file_path = hf_hub_download(repo_id=HF_DATASET_NAME, filename=\"dataset.parquet\", repo_type=\"dataset\")\n",
        "print(f\"Successfully downloaded 'dataset.parquet' to: {downloaded_file_path}\")\n",
        "\n",
        "df = pd.read_parquet(downloaded_file_path)\n",
        "print(f\"Successfully read Parquet file with {len(df)} entries.\")\n",
        "\n",
        "if df is not None and not df.empty:\n",
        "    print(f\"Successfully loaded data from Hugging Face Hub ({len(df)} entries).\")\n",
        "else:\n",
        "    print(f\"All attempts to load from Hugging Face Hub ({HF_DATASET_NAME}) failed to populate DataFrame.\")"
      ],
      "metadata": {
        "id": "smkB90xqSYxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Option B: Loading from local Parquet or CSV\n",
        "LOCAL_PARQUET_PATH = \"\"\n",
        "LOCAL_CSV_PATH = \"\"\n",
        "\n",
        "if os.path.exists(LOCAL_PARQUET_PATH):\n",
        "    print(f\"Loading data from local Parquet file: {LOCAL_PARQUET_PATH}\")\n",
        "    try:\n",
        "        df = pd.read_parquet(LOCAL_PARQUET_PATH)\n",
        "        print(f\"Successfully loaded {len(df)} entries from local Parquet file.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading from local Parquet file: {e}\")\n",
        "elif os.path.exists(LOCAL_CSV_PATH):\n",
        "    print(f\"Loading data from local CSV file: {LOCAL_CSV_PATH}\")\n",
        "    try:\n",
        "        df = pd.read_csv(LOCAL_CSV_PATH)\n",
        "        print(f\"Successfully loaded {len(df)} entries from local CSV file.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading from local CSV file: {e}\")\n",
        "else:\n",
        "    print(f\"Local Parquet/CSV files not found at expected paths: {LOCAL_PARQUET_PATH} or {LOCAL_CSV_PATH}\")"
      ],
      "metadata": {
        "id": "DDy6_ljwT4ZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Option C: Loading from SQLite\n",
        "DB_PATH = \"\"\n",
        "\n",
        "if os.path.exists(DB_PATH):\n",
        "    print(f\"Loading data from SQLite database: {DB_PATH}\")\n",
        "    try:\n",
        "        conn = sqlite3.connect(DB_PATH)\n",
        "\n",
        "        query = \"\"\"\n",
        "        SELECT\n",
        "            f.id AS formula_id,\n",
        "            f.formula AS ltl_formula_original,\n",
        "            f.spot_formulas,\n",
        "            f.canonical_form AS ltl_canonical_spot,\n",
        "            f.depth AS ltl_depth,\n",
        "            i.id AS itl_id,\n",
        "            i.itl_text,\n",
        "            i.canonical_form AS itl_is_canonical_rule_based,\n",
        "            n.id AS nl_id,\n",
        "            n.domain,\n",
        "            n.activity,\n",
        "            n.translation AS nl_translation\n",
        "        FROM\n",
        "            formulas f\n",
        "        LEFT JOIN -- Use LEFT JOIN in case some ITLs or NLs are missing for a formula\n",
        "            itl_representations i ON f.id = i.formula_id AND i.canonical_form = 1 -- Only canonical ITL\n",
        "        LEFT JOIN\n",
        "            nl_translations n ON f.id = n.formula_id AND i.id = n.itl_id -- Join on ITL ID too if NL is specific to ITL variant\n",
        "        WHERE i.id IS NOT NULL AND n.id IS NOT NULL; -- Ensure we only get complete triplets\n",
        "        \"\"\"\n",
        "        df = pd.read_sql_query(query, conn)\n",
        "        conn.close()\n",
        "        print(f\"Successfully loaded {len(df)} entries from SQLite database.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading from SQLite: {e}\")\n",
        "else:\n",
        "    print(f\"SQLite database not found at {DB_PATH}.\")"
      ],
      "metadata": {
        "id": "-7G4vDDFUOGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3 Basic Dataset Information\n",
        "\n",
        "Let's look at the first few rows, the shape of the dataset, and column information."
      ],
      "metadata": {
        "id": "uim9uvucUh2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataset Head:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nDataset Shape:\")\n",
        "print(df.shape)\n",
        "\n",
        "print(\"\\nDataset Columns:\")\n",
        "print(df.columns)\n",
        "\n",
        "print(\"\\nDataset Info:\")\n",
        "df.info()"
      ],
      "metadata": {
        "id": "L7C7RgtrUfQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.4 Data Distributions\n",
        "Let's visualize some key distributions.\n",
        "\n",
        "##### **Distribution of Domains**"
      ],
      "metadata": {
        "id": "6XzFQ3BIU1KG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not df.empty and 'domain' in df.columns:\n",
        "    plt.figure(figsize=(12, 7))\n",
        "    sns.countplot(data=df, y='domain', order=df['domain'].value_counts().index, palette='viridis')\n",
        "    plt.title('Distribution of Samples Across Domains')\n",
        "    plt.xlabel('Number of Samples')\n",
        "    plt.ylabel('Domain')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Cannot plot domain distribution: DataFrame is empty or 'domain' column is missing.\")"
      ],
      "metadata": {
        "id": "9HXY_U8dUy02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Distribution of Natural Language (NL) Translation Lengths**\n",
        "We can look at the word count of the NL translations."
      ],
      "metadata": {
        "id": "Xd13yUzZVCWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not df.empty and 'translation' in df.columns:\n",
        "    df['nl_word_count'] = df['translation'].astype(str).apply(lambda x: len(x.split()))\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(df['nl_word_count'], kde=True, bins=30)\n",
        "    plt.title('Distribution of NL Translation Word Counts')\n",
        "    plt.xlabel('Word Count')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nStatistics for NL Translation Word Counts:\")\n",
        "    print(df['nl_word_count'].describe())\n",
        "else:\n",
        "    print(\"Cannot plot NL translation lengths: DataFrame is empty or 'translation' column is missing.\")"
      ],
      "metadata": {
        "id": "Sh8Dwde2U_of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.5 Example Triplets\n",
        "Let's look at a few full examples of LTL-ITL-NL triplets."
      ],
      "metadata": {
        "id": "_-PnT7kZVUxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not df.empty:\n",
        "    print(\"\\n--- Example Triplets from the Dataset ---\")\n",
        "    num_examples_to_show = 3\n",
        "    sample_size = min(num_examples_to_show, len(df))\n",
        "    if sample_size > 0:\n",
        "        for i, (_, row) in enumerate(df.sample(n=sample_size, random_state=42).iterrows()):\n",
        "            print(f\"\\nExample #{i+1}:\")\n",
        "            print(f\"  Domain: {row.get('domain', 'N/A')}\")\n",
        "            # Prefer spot_formulas if available, fallback to ltl_formula_original\n",
        "            ltl_display = row.get('spot_formulas', row.get('formula', 'N/A'))\n",
        "            print(f\"  LTL (Spot/Original): {ltl_display}\")\n",
        "            print(f\"  ITL: {row.get('itl_text', 'N/A')}\")\n",
        "            print(f\"  Activity Context: {row.get('activity', 'N/A')}\")\n",
        "            print(f\"  NL Translation: {row.get('translation', 'N/A')}\")\n",
        "            print(\"-\" * 40)\n",
        "    else:\n",
        "        print(\"No data available to show examples.\")\n",
        "else:\n",
        "    print(\"DataFrame is empty. Cannot show examples.\")"
      ],
      "metadata": {
        "id": "apbF4Y5nVYsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Data exploration notebook finished.\")"
      ],
      "metadata": {
        "id": "PhLO9hCsVtQb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}